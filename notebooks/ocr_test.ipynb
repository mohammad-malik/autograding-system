{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# PDF OCR Test Notebook\n",
        "\n",
        "This notebook demonstrates the PDF text recognition capabilities using MistralAI's OCR service.\n",
        "\n",
        "We'll test the OCR pipeline with a sample PDF document to verify the functionality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "from mistralai import Mistral, DocumentURLChunk, TextChunk\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Get API key from environment\n",
        "api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
        "if not api_key:\n",
        "    print(\"Warning: MISTRAL_API_KEY not found in environment variables.\")\n",
        "    api_key = \"your_api_key_here\"  # Replace with your actual key when testing\n",
        "\n",
        "# Initialize Mistral client\n",
        "client = Mistral(api_key=api_key)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_pdf(file_path):\n",
        "    \"\"\"\n",
        "    Process PDF document using MistralAI's OCR service\n",
        "    \n",
        "    Args:\n",
        "        file_path: Path to the PDF file\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary containing OCR results\n",
        "    \"\"\"\n",
        "    # Verify PDF file exists\n",
        "    pdf_file = Path(file_path)\n",
        "    if not pdf_file.is_file():\n",
        "        raise FileNotFoundError(f\"PDF file not found: {file_path}\")\n",
        "        \n",
        "    try:\n",
        "        # Upload PDF file to Mistral's OCR service\n",
        "        uploaded_file = client.files.upload(\n",
        "            file={\n",
        "                \"file_name\": pdf_file.stem,\n",
        "                \"content\": pdf_file.read_bytes(),\n",
        "            },\n",
        "            purpose=\"ocr\",\n",
        "        )\n",
        "        \n",
        "        # Get URL for the uploaded file\n",
        "        signed_url = client.files.get_signed_url(file_id=uploaded_file.id, expiry=1)\n",
        "        \n",
        "        # Process PDF with OCR\n",
        "        pdf_response = client.ocr.process(\n",
        "            document=DocumentURLChunk(document_url=signed_url.url),\n",
        "            model=\"mistral-ocr-latest\",\n",
        "            include_image_base64=True,\n",
        "        )\n",
        "        \n",
        "        # Convert response to dictionary\n",
        "        response_dict = json.loads(pdf_response.model_dump_json())\n",
        "        \n",
        "        return pdf_response, response_dict\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing PDF: {str(e)}\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "def extract_text_from_response(ocr_response):\n",
        "    \"\"\"\n",
        "    Extract text from OCR response\n",
        "    \n",
        "    Args:\n",
        "        ocr_response: OCR response object\n",
        "    \n",
        "    Returns:\n",
        "        Extracted text\n",
        "    \"\"\"\n",
        "    if not ocr_response:\n",
        "        return \"\"\n",
        "        \n",
        "    extracted_text = \"\"\n",
        "    for page in ocr_response.pages:\n",
        "        extracted_text += page.text + \"\\n\\n\"\n",
        "    \n",
        "    return extracted_text.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with a sample PDF\n",
        "# Replace this with your actual test PDF path\n",
        "pdf_path = \"sample_document.pdf\"\n",
        "\n",
        "# Check if we need to create a sample PDF\n",
        "if not os.path.exists(pdf_path):\n",
        "    print(f\"Warning: Test PDF {pdf_path} not found.\")\n",
        "    print(\"Please create a sample PDF document for testing.\")\n",
        "    print(\"You can use any PDF document with text content.\")\n",
        "    \n",
        "    # For demonstration purposes, we'll just set a flag to skip processing\n",
        "    skip_processing = True\n",
        "else:\n",
        "    skip_processing = False\n",
        "\n",
        "# Process the PDF\n",
        "if not skip_processing and api_key != \"your_api_key_here\":\n",
        "    print(f\"Processing PDF: {pdf_path}\")\n",
        "    ocr_response, response_dict = process_pdf(pdf_path)\n",
        "    \n",
        "    if ocr_response:\n",
        "        # Extract and display text\n",
        "        extracted_text = extract_text_from_response(ocr_response)\n",
        "        print(\"\\n--- Extracted Text (First 500 chars) ---\")\n",
        "        print(extracted_text[:500] + \"...\")\n",
        "        \n",
        "        # Print some metadata\n",
        "        print(\"\\n--- Document Metadata ---\")\n",
        "        print(f\"Number of pages: {len(ocr_response.pages)}\")\n",
        "        \n",
        "        # Print first part of the raw response\n",
        "        print(\"\\n--- Raw Response (First 1000 chars) ---\")\n",
        "        print(json.dumps(response_dict, indent=4)[:1000] + \"...\")\n",
        "    else:\n",
        "        print(\"Failed to process PDF.\")\n",
        "else:\n",
        "    print(\"Skipping PDF processing. Please set your API key and provide a PDF file to test.\")\n",
        "    extracted_text = \"Sample extracted text (API key not provided or PDF not found)\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from mistralai.models import OCRResponse\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "\n",
        "def replace_images_in_markdown(markdown_str: str, images_dict: dict) -> str:\n",
        "    \"\"\"\n",
        "    Replace image placeholders in markdown with base64-encoded images.\n",
        "\n",
        "    Args:\n",
        "        markdown_str: Markdown text containing image placeholders\n",
        "        images_dict: Dictionary mapping image IDs to base64 strings\n",
        "\n",
        "    Returns:\n",
        "        Markdown text with images replaced by base64 data\n",
        "    \"\"\"\n",
        "    for img_name, base64_str in images_dict.items():\n",
        "        markdown_str = markdown_str.replace(\n",
        "            f\"![{img_name}]({img_name})\", f\"![{img_name}]({base64_str})\"\n",
        "        )\n",
        "    return markdown_str\n",
        "\n",
        "\n",
        "def get_combined_markdown(ocr_response: OCRResponse) -> str:\n",
        "    \"\"\"\n",
        "    Combine OCR text and images into a single markdown document.\n",
        "\n",
        "    Args:\n",
        "        ocr_response: Response from OCR processing containing text and images\n",
        "\n",
        "    Returns:\n",
        "        Combined markdown string with embedded images\n",
        "    \"\"\"\n",
        "    markdowns: list[str] = []\n",
        "    # Extract images from page\n",
        "    for page in ocr_response.pages:\n",
        "        image_data = {}\n",
        "        for img in page.images:\n",
        "            image_data[img.id] = img.image_base64\n",
        "        # Replace image placeholders with actual images\n",
        "        markdowns.append(replace_images_in_markdown(page.markdown, image_data))\n",
        "\n",
        "    return \"\\n\\n\".join(markdowns)\n",
        "\n",
        "\n",
        "# Display combined markdowns and images if we have a response\n",
        "if 'ocr_response' in locals() and ocr_response:\n",
        "    print(\"\\n--- Displaying PDF with embedded images ---\")\n",
        "    display(Markdown(get_combined_markdown(ocr_response)))\n",
        "else:\n",
        "    print(\"\\nNo OCR response available to display.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Integration with Grading System\n",
        "\n",
        "This section demonstrates how the OCR results from PDF documents would be integrated with the AI grading system.\n",
        "\n",
        "In a real application, the extracted text from the PDF would be sent to an LLM for grading against an answer key.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulated answer key\n",
        "answer_key = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Simulated LLM grading function\n",
        "def grade_answer(student_answer, correct_answer):\n",
        "    \"\"\"\n",
        "    Simulate LLM grading of a student answer\n",
        "    \n",
        "    In a real application, this would call the OpenAI API with a prompt\n",
        "    that includes the student answer and correct answer.\n",
        "    \n",
        "    Args:\n",
        "        student_answer: Text of student's answer\n",
        "        correct_answer: Text of correct answer\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with score, feedback, and justification\n",
        "    \"\"\"\n",
        "    # This is a simplified simulation\n",
        "    # In reality, we would use an LLM to compare the answers\n",
        "    \n",
        "    # Calculate a simple similarity score\n",
        "    # (This is just for demonstration - a real system would use the LLM)\n",
        "    student_words = set(student_answer.lower().split())\n",
        "    correct_words = set(correct_answer.lower().split())\n",
        "    \n",
        "    if not student_words:\n",
        "        return {\n",
        "            \"score\": 0,\n",
        "            \"feedback\": \"No answer provided.\",\n",
        "            \"justification\": \"The answer was blank.\"\n",
        "        }\n",
        "    \n",
        "    common_words = student_words.intersection(correct_words)\n",
        "    similarity = len(common_words) / len(correct_words) if correct_words else 0\n",
        "    score = min(100, int(similarity * 100))\n",
        "    \n",
        "    # Generate feedback based on score\n",
        "    if score >= 90:\n",
        "        feedback = \"Excellent answer! Your response is correct.\"\n",
        "        justification = \"The answer contains all the key elements of the correct response.\"\n",
        "    elif score >= 70:\n",
        "        feedback = \"Good answer, but missing some details.\"\n",
        "        justification = \"The answer contains most key elements but is missing some important details.\"\n",
        "    elif score >= 50:\n",
        "        feedback = \"Partial credit. Your answer is incomplete.\"\n",
        "        justification = \"The answer contains some correct elements but is missing significant content.\"\n",
        "    else:\n",
        "        feedback = \"Your answer needs improvement.\"\n",
        "        justification = \"The answer is missing most of the required elements.\"\n",
        "    \n",
        "    return {\n",
        "        \"score\": score,\n",
        "        \"feedback\": feedback,\n",
        "        \"justification\": justification\n",
        "    }\n",
        "\n",
        "# Grade the extracted text (using just the first 500 characters for demo purposes)\n",
        "if 'extracted_text' in locals():\n",
        "    student_answer = extracted_text[:500]  # Just use first 500 chars for demo\n",
        "    grade_result = grade_answer(student_answer, answer_key)\n",
        "    \n",
        "    # Display results\n",
        "    print(\"\\n--- Grading Results ---\")\n",
        "    print(f\"Student Answer (first 100 chars): {student_answer[:100]}...\")\n",
        "    print(f\"Correct Answer: {answer_key}\")\n",
        "    print(f\"Score: {grade_result['score']}%\")\n",
        "    print(f\"Feedback: {grade_result['feedback']}\")\n",
        "    print(f\"Justification: {grade_result['justification']}\")\n",
        "else:\n",
        "    print(\"\\n--- Grading Results ---\")\n",
        "    print(\"No extracted text available for grading.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook demonstrates the basic workflow of the PDF OCR and grading system:\n",
        "\n",
        "1. PDF document processing using MistralAI's OCR service\n",
        "2. Text extraction from PDF content\n",
        "3. Grading the extracted text against an answer key\n",
        "\n",
        "In the full application, this process would be integrated into the FastAPI backend, with the following enhancements:\n",
        "\n",
        "- Robust error handling and validation\n",
        "- Integration with the Pinecone vector database for textbook content\n",
        "- Advanced LLM-based grading with chain-of-thought reasoning\n",
        "- Detailed feedback generation for students\n",
        "- Support for various PDF formats and structures\n",
        "\n",
        "The PDF OCR and grading system is a key component of the autograding system, enabling automated assessment of student quiz responses submitted as PDF documents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "odfkuCk6qSAw"
      },
      "outputs": [],
      "source": [
        "# Initialize Mistral client with API key\n",
        "from mistralai import Mistral, DocumentURLChunk, TextChunk\n",
        "import os\n",
        "\n",
        "\n",
        "client = Mistral(api_key=os.getenv(\"MISTRAL_API_KEY\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files in bucket: ['23i-0033-6.pdf', '23i-0047-6.pdf', '23i-0122-6.pdf', '23i-0125-6.pdf', '23i-0126-6.pdf', '23i-0127-6.pdf', '23i-2005-6.pdf', '23i-2084-6.pdf']\n",
            "Downloaded successfully!\n"
          ]
        }
      ],
      "source": [
        "from supabase import create_client\n",
        "import os\n",
        "\n",
        "\n",
        "supabase_url = os.getenv(\"SUPABASE_URL\")\n",
        "SUPABASE_SERVICE_ROLE_KEY = os.getenv(\"SUPABASE_SERVICE_ROLE_KEY\")\n",
        "\n",
        "supabase = create_client(supabase_url, SUPABASE_SERVICE_ROLE_KEY)\n",
        "\n",
        "# Now you can list and download from Storage:\n",
        "bucket_name = \"database-pdfs\"\n",
        "file_path = \"23i-2005-6.pdf\"\n",
        "\n",
        "# List files (optional, for debugging)\n",
        "files = supabase.storage.from_(bucket_name).list()\n",
        "print(\"Files in bucket:\", [f[\"name\"] for f in files])\n",
        "\n",
        "# Download file\n",
        "file_bytes = supabase.storage.from_(bucket_name).download(file_path)\n",
        "with open(file_path, \"wb\") as f:\n",
        "    f.write(file_bytes)\n",
        "print(\"Downloaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svaJGBFlqm7_",
        "outputId": "d59c5b68-486c-41a3-a0b9-8f6c0baa14d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"pages\": [\n",
            "        {\n",
            "            \"index\": 0,\n",
            "            \"markdown\": \"# 1.Abstract \\n\\nThis project proposes the development of a simple AI-powered educational system that helps teachers create content and grade student work automatically. The system consists of two main modules designed to reduce teachers' workload and provide faster feedback to students.\\n\\n### 1.1 Module 1: Content Generation\\n\\nTeachers upload textbook PDFs which are processed and stored in a Pinecone vector database. Teachers then write simple prompts describing what they need (like \\\"create slides for Vision Transformer\\\" or \\\"make a quiz on Vision Transformer\\\"). The system will retrieve relevant content from the vector database that will serve as context for the Large Language Model (LLM) for PowerPoint slides and quiz generation based on teachers' requests.\\n\\n### 1.2 Module 2: Quiz Grading\\n\\nStudents submit photos of their handwritten quiz responses. Teaching assistants input the correct answers throug\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "\n",
        "# Verify PDF file exists\n",
        "pdf_file = Path(file_path)\n",
        "assert pdf_file.is_file()\n",
        "\n",
        "# Upload PDF file to Mistral's OCR service\n",
        "uploaded_file = client.files.upload(\n",
        "    file={\n",
        "        \"file_name\": pdf_file.stem,\n",
        "        \"content\": pdf_file.read_bytes(),\n",
        "    },\n",
        "    purpose=\"ocr\",\n",
        ")\n",
        "\n",
        "# Get URL for the uploaded file\n",
        "signed_url = client.files.get_signed_url(file_id=uploaded_file.id, expiry=1)\n",
        "\n",
        "# Process PDF with OCR, including embedded images\n",
        "pdf_response = client.ocr.process(\n",
        "    document=DocumentURLChunk(document_url=signed_url.url),\n",
        "    model=\"mistral-ocr-latest\",\n",
        "    include_image_base64=True,\n",
        ")\n",
        "\n",
        "# Convert response to JSON format\n",
        "response_dict = json.loads(pdf_response.model_dump_json())\n",
        "\n",
        "print(json.dumps(response_dict, indent=4)[0:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"pages\": [\n",
            "        {\n",
            "            \"index\": 0,\n",
            "            \"markdown\": \"# Database Systems\\n\\nDate: 6th May 2025 QUIZ 6\\n\\nName: M. Ahmed Riaz Roll No: 23I-2005 Marks: 4/10\\n\\nQ: Consider the relation below with the given functional dependencies. Assume no other FDs exist. Answer the following questions.\\n\\n## Book Loans\\n\\n|  Member_ID | Book_ISBN | Loan_Date | Return_Date | Member_Type | Book_Title | Author | Late Fee Rate  |\\n| --- | --- | --- | --- | --- | --- | --- | --- |\\n|  |   |   |   |   |   |   |   |\\n\\n- FD1: Member_ID, Book_ISBN \\u2192 Loan_Date, Return_Date\\n- FD2: Member_Type \\u2192 Late_Fee_Rate\\n- FD3: Book_ISBN \\u2192 Book_Title, Author\\n- FD4: Book_ISBN \\u2192 Late_Fee_Rate\\n\\nCandidate Key: {Member_ID, Book_ISBN}\\n\\na. Is the relation in 2NF? Provide the reason if it is not in 2NF. [2 marks] The relation is not in 2NF, because in there is partial dependency exists in Functional Dependency. FD1. We have to create those table to be in 2NF.\\n\\nb. Is the relation in\n"
          ]
        }
      ],
      "source": [
        "# Upload PDF file to Mistral's OCR service\n",
        "uploaded_file = client.files.upload(\n",
        "    file={\n",
        "        \"file_name\": pdf_file.stem,\n",
        "        \"content\": pdf_file.read_bytes(),\n",
        "    },\n",
        "    purpose=\"ocr\",\n",
        ")\n",
        "\n",
        "# Get URL for the uploaded file\n",
        "signed_url = client.files.get_signed_url(file_id=uploaded_file.id, expiry=1)\n",
        "\n",
        "# Process PDF with OCR, including embedded images\n",
        "pdf_response = client.ocr.process(\n",
        "    document=DocumentURLChunk(document_url=signed_url.url),\n",
        "    model=\"mistral-ocr-latest\",\n",
        "    include_image_base64=True,\n",
        ")\n",
        "\n",
        "# Convert response to JSON format\n",
        "response_dict = json.loads(pdf_response.model_dump_json())\n",
        "print(json.dumps(response_dict, indent=4)[0:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dxefUpm-Idp8",
        "outputId": "715b1d4b-5afb-4b96-e15d-ec3fc1b287b5"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "# 1.Abstract \n",
              "\n",
              "This project proposes the development of a simple AI-powered educational system that helps teachers create content and grade student work automatically. The system consists of two main modules designed to reduce teachers' workload and provide faster feedback to students.\n",
              "\n",
              "### 1.1 Module 1: Content Generation\n",
              "\n",
              "Teachers upload textbook PDFs which are processed and stored in a Pinecone vector database. Teachers then write simple prompts describing what they need (like \"create slides for Vision Transformer\" or \"make a quiz on Vision Transformer\"). The system will retrieve relevant content from the vector database that will serve as context for the Large Language Model (LLM) for PowerPoint slides and quiz generation based on teachers' requests.\n",
              "\n",
              "### 1.2 Module 2: Quiz Grading\n",
              "\n",
              "Students submit photos of their handwritten quiz responses. Teaching assistants input the correct answers through the system. The artificial intelligence uses Optical Character Recognition (OCR) technology to read the handwritten material, cross-checks it with the correct answers through large language model (LLM) methods, and marks each submission automatically. The system generates personalized reports for each student after this process, displaying their marks, correct answers, and personalized comments.\n",
              "\n",
              "The main benefits are enormous teacher time savings, faster feedback to students, uniform grading, and larger class sizes without additional staff. The system uses simple web interfaces that don't require technical expertise, so it's accessible to all teachers with or without a technology background. Expected outcomes are to reduce teacher preparation time by $60-70 \\%$, provide same-day student feedback instead of waiting a week, and improve learning outcomes through timely, detailed feedback reports.\n",
              "\n",
              "## 2. Introduction and Background\n",
              "\n",
              "### 2.1 Stakeholders and Topics Concerned\n",
              "\n",
              "## Primary Stakeholders:\n",
              "\n",
              "Teachers and Educators:Teachers and Educators struggle with content creation and grading quizzes which is also time consuming. They need a tool that makes their life easy and does not compromise education content delivery.\n",
              "Students: Students require well structured educational material and timely feedback. Waiting weeks for grading of their tasks slows the learning process.\n",
              "Teaching Assistants (TAs): Graduate or junior students who spend time grading the tasks under supervision of a faculty member.\n",
              "\n",
              "Educational Institutions: Educational institutions that want to improve teacher efficiency and manage increasing student population without compromising educational content delivery.\n",
              "\n",
              "# 2.2 Problem Statement \n",
              "\n",
              "The core problem is the inefficient use of educator time due to manual, repetitive tasks that could be automated:\n",
              "\n",
              "## Content Creation Challenges:\n",
              "\n",
              "- Educators often report needing 3-8 hours per lecture hour during initial development stages Ideas to Reduce Non-Classroom Prep Time Without Reducing Teaching Quality - ISTLDSimon Fraser University. So, converting a chapter into a polished slide deck can realistically take 3-8 hours, especially when Structuring content, Designing visuals and Drafting speaker notes.\n",
              "- Crafting quizzes requires extensive planning and formatting, especially when aligning with learning objectives and varying question types.\n",
              "- Quality of content fluctuates widely, with some instructors delivering rich, detailed material and others creating only minimal slide decks.\n",
              "- Extracting and reformatting textbook content remains a time-consuming manual task, often requiring copy-paste and slide/template adjustments.\n",
              "\n",
              "\n",
              "## Assessment and Grading Issues:\n",
              "\n",
              "Handwritten quiz grading takes 2-3 minutes per student per quiz\n",
              "Large classes (100+ students) create week-long grading delays\n",
              "Grading consistency varies between different graders\n",
              "Providing detailed feedback is time-consuming but crucial for learning\n",
              "Students receive feedback too late to be useful for improvement\n",
              "\n",
              "## Resource Constraints:\n",
              "\n",
              "Limited teaching assistant availability for grading support\n",
              "Pressure to handle larger class sizes with same staff\n",
              "Need for scalable solutions that maintain educational quality\n",
              "\n",
              "### 2.3 Project Importance\n",
              "\n",
              "This project addresses systemic challenges in UK higher education, supporting academic staff and improving student learning through automation.\n",
              "\n",
              "Scale of Impact: According to HESA (2023), there are over 228,000 academic staff employed in UK higher education. Streamlining routine tasks such as grading and slide creation-even by just 30 minutes per week per lecturer-could result in a nationwide time saving of over 5 million hours annually, freeing time for research, mentoring, and pedagogy. Source: Higher Education Staff Statistics: UK, 2021/22 | HESA\n",
              "\n",
              "Effectiveness in Learning: The Education Endowment Foundation (EEF) emphasizes that high-quality feedback is one of the most effective strategies for improving learning, particularly when it is timely and actionable. Their 2021 guidance report notes that delayed feedback weakens impact, as students are less able to connect it to the learning task. Integrating automation allows for faster, scalable feedback, enhancing academic outcomes.\n",
              "\n",
              "# Educational Equity: Large modules in UK universities often involve multiple markers, leading to variation in grading quality. Automation helps standardise assessment criteria, ensuring fairness and consistency regardless of class size or instructor workload. This is particularly important for first-year undergraduate courses, where large class sizes and uneven marking are common. \n",
              "\n",
              "Sustainability: As student numbers increase and education budgets tighten, automation is the answer to upholding quality education.\n",
              "\n",
              "### 2.4 Benefits to Stakeholders\n",
              "\n",
              "## For Teachers:\n",
              "\n",
              "Teachers using AI tools for grading, quiz creation, and content planning report weekly time savings equivalent to $\\mathbf{6}$ hours on average. Source: Three in 10 Teachers Use AI Weekly, Saving Six Weeks a Year\n",
              "Focus more time on actual teaching and student interaction\n",
              "Maintain consistent grading standards across all assignments\n",
              "Easily create multiple versions of quizzes to prevent cheating\n",
              "Generate comprehensive analytics on student performance\n",
              "\n",
              "## For Students:\n",
              "\n",
              "Receive feedback within 24 hours instead of 1-2 weeks\n",
              "Get detailed explanations for incorrect answers\n",
              "Access to well-structured slides and study materials\n",
              "Consistent grading regardless of which TA grades their work\n",
              "Better learning outcomes through timely feedback\n",
              "\n",
              "## For Teaching Assistants:\n",
              "\n",
              "Target to reduce grading workload by $80-90 \\%$\n",
              "Focus on higher-value tasks like tutoring and research\n",
              "Provide input on answer keys rather than grading individual papers\n",
              "More time for their own academic work\n",
              "\n",
              "## For Educational Institutions:\n",
              "\n",
              "Handle larger class sizes without hiring additional staff\n",
              "Improve student satisfaction scores through faster feedback\n",
              "Reduce operational costs in the long term\n",
              "Maintain consistent quality across different instructors\n",
              "Generate data-driven insights for curriculum improvement\n",
              "\n",
              "### 2.5 Relationship to Previous Work and Existing Knowledge\n",
              "\n",
              "## Educational Technology Evolution:\n",
              "\n",
              "AI-assisted learning began with Computer-Assisted Instruction (CAI) in the 1960s, typified by systems like PLATO and the IBM 1500. These early architectures enabled adaptive problem sets and rudimentary learner modeling (PDF) The Development History and Future Trend of\n",
              "\n",
              "Computer-Assisted Teaching.\n",
              "In the 1970s and 1980s, Intelligent Tutoring Systems (ITSs) emerged, integrating domain knowledge, student modeling, and pedagogical strategies (e.g., Carbonell's SCHOLAR, Sleeman \\& Brown's PIXIE Intelligent tutoring systems: an overview.\n",
              "\n",
              "While modern LMS platforms like Moodle and Blackboard enable content delivery, there remains a significant gap in automating content creation and streamlined assessment-a gap that this project addresses.\n",
              "\n",
              "# Natural Language Processing in Education: \n",
              "\n",
              "Recent work has validated the use of LLMs for question generation. Elkins et al. (2024) demonstrated that GPT-generated quiz questions, designed using Bloom's taxonomy, were rated by teachers as comparable in quality to human-authored ones-with potential to even improve quizzes [2401.05914] How Teachers Can Use Large Language Models and Bloom's Taxonomy to Create Educational Quizzes. Additionally, Doughty et al. (2023) found GPT-4 produced clear, well-aligned MCQs in programming education, further supporting LLM viability for educational content creation [2312.03173] A Comparative Study of AI-Generated (GPT-4) and Human-crafted MCQs in Programming Education.\n",
              "\n",
              "## Optical Character Recognition Progress:\n",
              "\n",
              "Transformer-based OCR (TrOCR): Li et al. (2021) introduced TrOCR, a transformer-based OCR model that achieved state-of-the-art results on handwritten and printed text [2109.10282] TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models.\n",
              "\n",
              "TrOCR accuracy on handwritten data: A comprehensive 2024 evaluation found that TrOCR achieved $\\sim 95 \\%$ character accuracy on the IAM handwritten dataset A Comprehensive Evaluation of TrOCR with Varying Image Effects - NHSJS.\n",
              "\n",
              "## Vector Database Applications:\n",
              "\n",
              "The emergence of vector databases like Pinecone has revolutionized information retrieval. Educational applications are just beginning to explore these capabilities for intelligent content search and organization.\n",
              "\n",
              "## Automated Assessment Research:\n",
              "\n",
              "Studies comparing humans and LLM graders show promising outcomes. For example, a South African study reported average grade discrepancies of just $4 \\%$ between ChatGPT-4 and human markers across university scripts ai versus human graders: assessing the role of large language models in higher education.\n",
              "\n",
              "## Gap in Current Solutions:\n",
              "\n",
              "While individual components exist (OCR tools, LLM APIs, vector databases), no integrated system specifically addresses the combined workflow of content generation from textbooks and handwritten assessment grading. Most educational AI tools focus on either content creation OR grading, but not both in a unified system.\n",
              "\n",
              "## Integration with Existing Systems:\n",
              "\n",
              "Unlike previous attempts that required complete replacement of existing workflows, this project is designed to complement current teaching practices by automating specific time-intensive tasks while preserving teacher control and judgment.\n",
              "\n",
              "## 3. Aims and Objectives\n",
              "\n",
              "### 3.1 Primary Aim\n",
              "\n",
              "The primary aim of this project is to develop and deploy a user-friendly AI system that automates educational content creation and handwritten quiz grading. The target is reducing teacher workload by\n",
              "\n",
              "60-70\\% while maintaining educational quality and providing faster student feedback.\n",
              "\n",
              "# 3.2 Specific Objectives \n",
              "\n",
              "## Module 1: Intelligent Content Generation\n",
              "\n",
              "## Objective 3.2.1: Book Processing and Storage System\n",
              "\n",
              "- Develop a PDF upload system that accepts textbook files\n",
              "- Implement text extraction and chunking algorithms to break content into manageable sections\n",
              "- Create a Pinecone vector database integration that stores textbook content as searchable embeddings\n",
              "- Build content indexing that preserves chapter structure and topic relationships\n",
              "\n",
              "\n",
              "## Objective 3.2.2: Al-Powered Content Generation\n",
              "\n",
              "- Integrate Large Language Models (GPT-4 or similar) for content creation\n",
              "- Develop a prompt system where teachers can request specific content (e.g., \"Create 15 slides on Vision transformer\")\n",
              "- Build slide generation that creates PowerPoint presentations with proper formatting\n",
              "- Implement quiz generation that produces multiple question types (multiple choice, short answer, true/false)\n",
              "- Create export functionality for PowerPoint (.pptx) and Word (.docx) formats\n",
              "\n",
              "\n",
              "## Module 2: Automated Quiz Assessment\n",
              "\n",
              "## Objective 3.2.3: Handwritten Quiz Processing\n",
              "\n",
              "- Develop image upload system for handwritten quiz photos\n",
              "- Implement OCR (Optical Character Recognition) to convert handwriting to digital text\n",
              "- Create image preprocessing to improve handwriting recognition accuracy\n",
              "- Build confidence scoring to identify unclear handwriting that needs human review\n",
              "\n",
              "\n",
              "## Objective 3.2.4: Al-Driven Grading System\n",
              "\n",
              "- Develop answer key input system for teaching assistants\n",
              "- Implement LLM-based comparison between student answers and correct answers\n",
              "- Create intelligent grading that understands context, not just exact word matches\n",
              "- Build scoring algorithms that assign partial credit appropriately\n",
              "- Generate detailed feedback explaining correct answers and student mistakes\n",
              "\n",
              "\n",
              "## Objective 3.2.5: Reporting and Analytics\n",
              "\n",
              "- Create individual student reports showing scores, correct answers, and feedback\n",
              "- Develop class-wide analytics showing common mistakes and performance trends\n",
              "- Implement downloadable reports in PDF format\n",
              "- Build email notification system for automatic report distribution\n",
              "\n",
              "# 3.3 System Integration and User Experience \n",
              "\n",
              "Objective 3.3.1: User Interface Development\n",
              "\n",
              "Design simple web interface requiring no technical expertise\n",
              "Create separate dashboards for teachers, TAs, and students\n",
              "Implement user authentication and role-based access control\n",
              "Build mobile-responsive design for accessibility\n",
              "Objective 3.3.2: System Performance and Reliability\n",
              "Ensure processing time under 5 minutes for content generation\n",
              "Achieve 24-hour turnaround for quiz grading\n",
              "Implement $99.5 \\%$ system uptime\n",
              "Create backup and recovery systems\n",
              "\n",
              "### 3.4 Intermediate Deliverables\n",
              "\n",
              "## Phase 1: Foundation Components\n",
              "\n",
              "Basic PDF upload and text extraction system\n",
              "Pinecone database setup with sample textbook content\n",
              "Simple OCR integration for handwriting recognition\n",
              "Basic user authentication system\n",
              "\n",
              "## Phase 2: Core Functionality\n",
              "\n",
              "Working slide generation from textbook content\n",
              "Quiz creation with multiple question types\n",
              "Handwritten text recognition with $90 \\%+$ accuracy\n",
              "Basic grading algorithm comparing answers\n",
              "\n",
              "## Phase 3: Integration and Enhancement\n",
              "\n",
              "Fully integrated content generation module\n",
              "Complete grading system with detailed feedback\n",
              "Web interface for all user types\n",
              "Export functionality for slides and reports\n",
              "\n",
              "## Phase 4: Testing and Deployment\n",
              "\n",
              "Beta testing with real teachers and students\n",
              "Performance optimization and bug fixes\n",
              "Complete documentation and user training materials\n",
              "Production deployment with monitoring systems\n",
              "\n",
              "### 3.5 Enabling Deliverables\n",
              "\n",
              "# Technical Infrastructure: \n",
              "\n",
              "Cloud server setup (AWS/Google Cloud) for hosting the system\n",
              "Database architecture supporting both vector and traditional data\n",
              "API endpoints connecting frontend interface to AI services\n",
              "Security implementation protecting student data and educational content\n",
              "\n",
              "## AI Model Integration:\n",
              "\n",
              "Fine-tuned prompts for educational content generation\n",
              "Handwriting recognition model optimized for student work\n",
              "Grading algorithms calibrated with teacher feedback\n",
              "Error handling for edge cases in AI processing\n",
              "\n",
              "## Data Pipeline:\n",
              "\n",
              "Automated text processing workflow from PDF to searchable content\n",
              "Image processing pipeline for handwritten quiz optimization\n",
              "Report generation templates for consistent formatting\n",
              "Backup systems ensuring no data loss\n",
              "\n",
              "## User Support Systems:\n",
              "\n",
              "User documentation and tutorial videos\n",
              "Help desk system for technical support\n",
              "Training materials for teachers and TAs\n",
              "Feedback collection system for continuous improvement\n",
              "\n",
              "### 3.6 Success Metrics\n",
              "\n",
              "## Efficiency Metrics:\n",
              "\n",
              "Reduce slide creation time from hours to minutes per chapter\n",
              "Decrease quiz grading time from 2-3 minutes to 10 seconds per student\n",
              "Achieve same-day feedback delivery instead of 7-14 day delays\n",
              "\n",
              "## Quality Metrics:\n",
              "\n",
              "Maintain 90\\%+ teacher satisfaction with generated content quality\n",
              "Achieve 85\\%+ correlation between AI grades and human grader assessment\n",
              "Reach $95 \\%+$ accuracy in handwriting recognition\n",
              "\n",
              "# 4. Literature Review \n",
              "\n",
              "### 4.1 Al in Educational Content Generation\n",
              "\n",
              "Quiz question generation with GPT-4: Elkins et al. (2024) showed that GPT-4 can produce multiple-choice questions aligned to Bloom's Taxonomy, with educator ratings indicating comparable quality to human-authored quizzes ChatGPT prompts for generating multiple-choice questions in medical education and evidence on their validity: a literature review.\n",
              "\n",
              "LLMs in programming education: Doughty et al. (2023) compared GPT-4-generated questions with human-crafted ones across 6 Python modules; they found Al questions to be clear, well-formed, and aligned with learning objectives [2312.03173] A Comparative Study of Al-Generated (GPT-4) and Human-crafted MCQs in Programming Education.\n",
              "\n",
              "Automated slide generation: Shreewastav et al. (2024) introduced \"Presently,\" a T5-based model converting research papers into PowerPoint slides. Compared to manual methods, Al-generated slides required less teacher intervention and were rated similarly in academic validity (PDF) Presently: Automated Presentation Slide Generation from Research Papers using NLP and Deep Learning (May 2024).\n",
              "\n",
              "### 4.2 Vector Databases and Information Retrieval\n",
              "\n",
              "Document-to-slide structuring: Mohan et al. (2021) created the \"D2S\" system using title-based retrieval plus summarization to convert academic papers into slides; human evaluation showed improved ROUGE scores D2S: Document-to-Slide Generation Via Query-Based Text Summarization - ACL Anthology.\n",
              "\n",
              "Pinecone-style databases: Though educational IR remains emergent, generic benchmarks (e.g. Kumar et al. 2023) show sub-100 ms retrieval latency over 10k+ document embeddingsâ€”promising for real-time educational workflows.\n",
              "\n",
              "### 4.3 OCR Technology and Handwriting Recognition\n",
              "\n",
              "Child handwriting evaluation: Alheraki et al. (2023) applied CNNs to child handwriting in Arabic, reaching high character recognition rates ( $\\sim 97 \\%$ ) and demonstrating feasibility in varied educational contexts Handwritten Arabic Character Recognition for Children Writing Using Convolutional Neural Network and Stroke Identification | Human-Centric Intelligent Systems.\n",
              "\n",
              "### 4.4 Automated Assessment and Grading Systems\n",
              "\n",
              "RATAS framework (Rubric Automated Tree-based Answer Scoring): Safilian et al. (2025) introduce a generative-Al-based system achieving high accuracy and interpretable, rubric-based grading on textual exam responses, closely matching human evaluation [2505.23818] Ratas framework: A comprehensive genai-based approach to rubric-based marking of real-world textual exams.\n",
              "\n",
              "Systematic review of automatic text assessment: Gao et al. (2023) analysed 93 studies on automatic grading of open-ended responses in higher education, demonstrating growing maturity and consistency in such systems [2308.16151] Automatic assessment of text-based responses in post-secondary education: A systematic review.\n",
              "\n",
              "### 4.5 Educational Technology Integration\n",
              "\n",
              "Teacher-centered AI integration (K-12): A study by researchers Ozan Filiz et al. (2025)\n",
              "\n",
              "identified key success factors for Al adoption, including usability and minimal disruption to routine workflows Teachers and Al: Understanding the factors influencing Al integration in K-12 education | Education and Information Technologies\n",
              "\n",
              "# 4.6 Gaps in Current Research \n",
              "\n",
              "While individual components of educational Al systems have been extensively studied, several gaps exist in current research:\n",
              "\n",
              "Integrated Systems: Most studies focus on isolated components (either content generation OR grading) rather than comprehensive systems that handle both tasks in a unified workflow.\n",
              "\n",
              "Handwritten Assessment Focus: Limited research exists on complete pipelines from handwritten quiz photos to final graded reports, particularly for diverse student populations.\n",
              "\n",
              "Real-world Implementation: Many studies report laboratory results but lack data on actual classroom implementation and long-term usage patterns.\n",
              "\n",
              "Teacher Workflow Integration: Few studies examine how Al tools integrate with existing teaching practices and institutional systems.\n",
              "\n",
              "### 4.7 Foundation for Current Project\n",
              "\n",
              "This literature review demonstrates that:\n",
              "\n",
              "1. Al content generation is technically feasible and educationally valuable\n",
              "2. Vector databases provide the necessary infrastructure for fast, relevant content retrieval\n",
              "3. OCR technology has reached sufficient accuracy for practical handwriting recognition\n",
              "4. Automated grading can achieve human-level performance with proper implementation\n",
              "5. Integration challenges require careful attention to user experience and workflow design\n",
              "\n",
              "The current project builds on these established foundations while addressing the identified gaps through:\n",
              "\n",
              "- Creating an integrated system combining content generation and assessment\n",
              "- Focusing specifically on handwritten quiz processing workflows\n",
              "- Designing for real classroom implementation rather than laboratory testing\n",
              "- Prioritizing teacher control and workflow compatibility\n",
              "\n",
              "This research foundation provides confidence that the proposed system is both technically achievable and educationally beneficial, while identifying specific areas where innovation and careful implementation will be crucial for success.\n",
              "\n",
              "## 5. Technologies and Resources\n",
              "\n",
              "### 5.1 Core Technologies\n",
              "\n",
              "Artificial Intelligence and Machine LearningLarge Language Models (LLMs):\n",
              "\n",
              "OpenAI GPT-4 API for content generation and quiz creation\n",
              "Claude API (Anthropic) as backup option for content generation\n",
              "Used for: Creating slides from textbook content, generating quiz questions, grading student answers, providing feedback\n",
              "\n",
              "# Optical Character Recognition (OCR): \n",
              "\n",
              "Google Cloud Vision API for handwritten text recognition\n",
              "TrOCR (Transformer-based OCR) for backup processing\n",
              "Used for: Converting handwritten quiz photos to digital text\n",
              "\n",
              "## Embedding Models:\n",
              "\n",
              "OpenAI text-embedding-ada-002 for creating text embeddings\n",
              "Sentence-BERT as alternative option\n",
              "Used for: Converting textbook content into searchable vectors\n",
              "\n",
              "### 5.2 Database and Storage Systems\n",
              "\n",
              "## Vector Database:\n",
              "\n",
              "Pinecone for storing and querying textbook embeddings\n",
              "Handles similarity search for content retrieval\n",
              "Estimated storage: 2 textbooks initially\n",
              "\n",
              "## Traditional Database:\n",
              "\n",
              "PostgreSQL for user accounts, grades, and structured data\n",
              "Stores user information, quiz results, system logs\n",
              "\n",
              "### 5.3 Web Development Framework\n",
              "\n",
              "## Backend Development:\n",
              "\n",
              "Python with FastAPI framework for building APIs\n",
              "Handles file uploads, Al integration, and data processing\n",
              "Automatic API documentation generation\n",
              "\n",
              "## Frontend Development:\n",
              "\n",
              "React.js with JavaScript for user interface\n",
              "Bootstrap CSS for responsive design\n",
              "Simple, user-friendly interface requiring no technical knowledge\n",
              "\n",
              "## Authentication and Security:\n",
              "\n",
              "JWT (JSON Web Tokens) for user authentication\n",
              "\n",
              "# 5.4 Cloud Infrastructure \n",
              "\n",
              "## Hosting Platform:\n",
              "\n",
              "Amazon Web Services (AWS) for cloud hosting\n",
              "Auto-scaling to handle varying user loads\n",
              "$99.9 \\%$ uptime guarantee\n",
              "\n",
              "## Server Specifications:\n",
              "\n",
              "4 vCPU, 16GB RAM for initial deployment\n",
              "500GB SSD storage for databases\n",
              "Additional GPU instances for AI processing when needed\n",
              "\n",
              "## Development Tools:\n",
              "\n",
              "Visual Studio Code for coding\n",
              "Git/GitHub for version control\n",
              "Docker for application packaging\n",
              "GitHub Actions for automated testing and deployment\n",
              "\n",
              "### 5.5 Data Sources and Requirements\n",
              "\n",
              "## Primary Data Sources:Educational Textbooks:\n",
              "\n",
              "Digital PDF versions of academic textbooks\n",
              "Target: 2 textbooks initially\n",
              "Sources: Open educational resources, partner institutions, public domain books\n",
              "Subjects: Machine Learning and Deep Learning\n",
              "\n",
              "## Handwriting Samples for Training:\n",
              "\n",
              "Handwritten quiz responses from volunteer students\n",
              "Diverse handwriting styles and quality levels\n",
              "Collected with proper consent and privacy protection\n",
              "\n",
              "## Assessment Standards:\n",
              "\n",
              "Grading rubrics from educational institutions\n",
              "Sample answer keys for various subjects\n",
              "Feedback templates for different question types\n",
              "Grading algorithm will be tested for bias using different handwritings.\n",
              "\n",
              "## Quality Requirements:\n",
              "\n",
              "High-resolution PDF scans (300 DPI minimum)\n",
              "Clear handwriting samples with good lighting\n",
              "Diverse content covering multiple educational levels\n",
              "All data properly anonymized for privacy\n",
              "\n",
              "# 5.6 Data Privacy and Compliance Resources \n",
              "\n",
              "## Legal and Privacy:\n",
              "\n",
              "Privacy policy and terms of service documentation\n",
              "GDPR compliance consultation\n",
              "Data encryption and security audit tools\n",
              "Backup and disaster recovery systems\n",
              "\n",
              "## Quality Assurance:\n",
              "\n",
              "Automated testing frameworks (pytest, Jest)\n",
              "Performance monitoring tools\n",
              "User feedback collection systems\n",
              "Content quality review processes\n",
              "\n",
              "### 5.7 Risk Mitigation Resources\n",
              "\n",
              "## Backup Technologies:\n",
              "\n",
              "Alternative AI providers (Claude, local models) if OpenAI becomes unavailable\n",
              "Multiple OCR services to ensure handwriting recognition reliability\n",
              "Backup hosting providers in case of AWS issues\n",
              "\n",
              "## Data Protection:\n",
              "\n",
              "Multiple backup systems across different geographic regions\n",
              "Version control for all code and configurations\n",
              "Regular security audits and penetration testing\n",
              "\n",
              "## Scalability Planning:\n",
              "\n",
              "Auto-scaling cloud infrastructure to handle growth\n",
              "Modular architecture allowing easy component upgrades\n",
              "Performance monitoring to identify bottlenecks early\n",
              "\n",
              "This technology stack provides a robust, scalable foundation for both content generation and automated grading while maintaining simplicity in user interaction and system administration.\n",
              "\n",
              "# 6. Method and Workplan \n",
              "\n",
              "### 6.1 Development Methodology\n",
              "\n",
              "This project will use an Agile development approach with 3-week sprints, allowing for continuous testing and improvement. The development will be divided into 4 major phases over 12 months, with each phase building upon the previous one.Key Principles: Build working prototypes early for teacher feedback\n",
              "Test each component with real users before moving forward\n",
              "Keep the system simple and user-friendly throughout development\n",
              "Regular stakeholder reviews to ensure educational needs are met\n",
              "\n",
              "### 6.2 Phase 1: Foundation Setup\n",
              "\n",
              "Goals: Establish basic infrastructure and test core technologies\n",
              "Sprint 1: System Design and Setup\n",
              "Set up AWS cloud infrastructure and databases\n",
              "Create basic user authentication system\n",
              "Design database schemas for users, content, and grades\n",
              "Set up development environment and code repositories\n",
              "\n",
              "## Sprint 2: PDF Processing and Storage\n",
              "\n",
              "Build PDF upload system for textbook files\n",
              "Implement text extraction from PDF documents\n",
              "Create Pinecone vector database integration\n",
              "Test with 5-10 sample textbook chapters\n",
              "\n",
              "## Sprint 3: OCR Integration and Testing\n",
              "\n",
              "$\\square$ Integrate Google Cloud Vision API for handwriting recognition\n",
              "Build image upload system for quiz photos\n",
              "Test OCR accuracy with 100+ handwriting samples\n",
              "Create confidence scoring for unclear handwriting\n",
              "\n",
              "## Sprint : Basic AI Integration\n",
              "\n",
              "Connect OpenAI GPT-4 API for content generation\n",
              "Build simple prompt system for slide creation\n",
              "Test LLM integration with sample textbook content\n",
              "Create basic grading comparison algorithms\n",
              "\n",
              "Working cloud infrastructure\n",
              "PDF upload and text extraction system\n",
              "Basic OCR handwriting recognition\n",
              "Simple AI content generation prototype\n",
              "$50+$ handwriting samples collected for testing\n",
              "\n",
              "# 6.3 Phase 2: Core Module Development \n",
              "\n",
              "Goals: Build the two main system modules with basic functionality\n",
              "Sprint 5: Content Generation Module\n",
              "Implement vector search for relevant textbook content\n",
              "Build slide generation with proper PowerPoint formatting\n",
              "Create quiz question generation (multiple choice, short answer)\n",
              "Add content difficulty assessment\n",
              "\n",
              "## Sprint 6 : Export and Template Systems\n",
              "\n",
              "Develop PowerPoint and Word document export\n",
              "Create customizable slide templates\n",
              "Build quiz formatting for different question types\n",
              "Test content generation with 3 pilot teachers\n",
              "\n",
              "## Sprint 7: Grading System Development\n",
              "\n",
              "Build answer key input system for teaching assistants\n",
              "Implement LLM-based answer comparison algorithms\n",
              "Create partial credit assignment logic\n",
              "Test grading accuracy with sample quiz responses\n",
              "\n",
              "## Sprint 8: Report Generation\n",
              "\n",
              "Develop individual student report templates\n",
              "Create class analytics and performance summaries\n",
              "Build PDF report generation system\n",
              "Test reporting with pilot student groups\n",
              "\n",
              "## Phase 2 Deliverables:\n",
              "\n",
              "Functional content generation creating slides and quizzes\n",
              "Working handwriting recognition and grading system\n",
              "Basic report generation for student feedback\n",
              "Testing results from 3 pilot teachers and 20+ students\n",
              "\n",
              "### 6.4 Phase 3: Integration and User Interface (Months 4-5)\n",
              "\n",
              "Goals: Create complete user experience and integrate all components\n",
              "Sprint 9: Web Interface Development\n",
              "Build React.js frontend for teachers, TAs, and students\n",
              "Create separate dashboards for different user roles\n",
              "Implement file upload interfaces and progress tracking\n",
              "Design mobile-responsive interface\n",
              "\n",
              "# Sprint 10: System Integration \n",
              "\n",
              "Connect frontend to all backend services\n",
              "Implement real-time processing status updates\n",
              "Create error handling and user feedback systems\n",
              "Build notification system for completed tasks\n",
              "\n",
              "## Sprint 11: Advanced Features\n",
              "\n",
              "Add customizable prompts for content generation\n",
              "Implement batch processing for multiple quizzes\n",
              "Create teacher review and editing capabilities\n",
              "Build analytics dashboard for institutional use\n",
              "\n",
              "## Sprint 12: Beta Testing\n",
              "\n",
              "Deploy beta version to pilot schools\n",
              "Conduct user training sessions with teachers\n",
              "Collect feedback and usage analytics\n",
              "Fix bugs and improve user experience\n",
              "\n",
              "## Phase 3 Deliverables:\n",
              "\n",
              "Complete web interface for all user types\n",
              "Fully integrated system with all features working\n",
              "Beta testing results from 5+ teachers and 100+ students\n",
              "User training materials and documentation\n",
              "\n",
              "### 6.5 Phase 4: Testing and Deployment (Month 6)\n",
              "\n",
              "Goals: Optimize performance, ensure reliability, and deploy production system\n",
              "Sprint 13: Performance Optimization\n",
              "Optimize database queries and Al processing speed\n",
              "Implement caching for frequently accessed content\n",
              "Test system with larger user loads\n",
              "Improve response times and system reliability\n",
              "Sprint 14: Security and Compliance\n",
              "Implement comprehensive data encryption\n",
              "Conduct security audit and penetration testing\n",
              "Ensure FERPA compliance for student data\n",
              "\n",
              "# Sprint 15: Production Deployment \n",
              "\n",
              "Deploy production system with monitoring\n",
              "Set up automated backup and scaling systems\n",
              "Create admin dashboard for system management\n",
              "Implement usage analytics and reporting\n",
              "\n",
              "## Sprint 16: Documentation and Training\n",
              "\n",
              "Complete user documentation and video tutorials\n",
              "Train support staff for help desk operations\n",
              "Create administrator guides for system maintenance\n",
              "Conduct final user acceptance testing\n",
              "\n",
              "## Phase 4 Deliverables:\n",
              "\n",
              "Production-ready system with $99.5 \\%$ uptime\n",
              "Complete security audit and compliance certification\n",
              "User training materials and support documentation\n",
              "Successfully processed 500+ student assignments\n",
              "\n",
              "### 6.6 Project Timeline and Major Milestones\n",
              "\n",
              "|  | Week 1 | Week 2 | Week 3 | Week 4 | Week 5 | Week 6 |\n",
              "| :-- | :-- | :-- | :-- | :-- | :-- | :-- |\n",
              "| Phase 1: <br> Foundation <br> Setup |  |  |  |  |  |  |\n",
              "| Phase 2: Core <br> Module <br> Development |  |  |  |  |  |  |\n",
              "| Phase 3: <br> Integration <br> and User <br> Interface |  |  |  |  |  |  |\n",
              "| Phase 4: <br> Testing and <br> Deployment |  |  |  |  |  |  |\n",
              "\n",
              "## Major Milestones:\n",
              "\n",
              "## Milestone\n",
              "\n",
              "M1: Foundation\n",
              "\n",
              "Month\n",
              "Success Criteria\n",
              "\n",
              "1 PDF processing, OCR working, basic AI\n",
              "\n",
              "# Complete Integration\n",
              "\n",
              "M2: Core Features\n",
              "Ready\n",
              "3\n",
              "Content generation and grading\n",
              "modules functional\n",
              "\n",
              "M3: Beta System\n",
              "Live\n",
              "5\n",
              "Complete user interface, pilot testing\n",
              "underway\n",
              "\n",
              "M4: Production\n",
              "Launch\n",
              "6\n",
              "Full deployment, user training complete\n",
              "\n",
              "6.7 Risk Analysis and Contingency Plans\n",
              "\n",
              "High-Risk Items:\n",
              "\n",
              "Risk 1: OCR Accuracy Below 90%\n",
              "\n",
              "*   *Impact:* Grading system unreliable, requires too much human correction\n",
              "*   *Probability:* Medium (30%)\n",
              "*   *Mitigation:* Test multiple OCR services, implement confidence scoring\n",
              "*   *Contingency:* Use crowd-sourced correction, partner with specialized OCR providers\n",
              "\n",
              "Risk 2: AI Content Quality Inconsistent\n",
              "\n",
              "*   *Impact:* Teachers reject generated slides/quizzes, system adoption fails\n",
              "*   *Probability:* Medium (25%)\n",
              "*   *Mitigation:* Extensive prompt engineering, teacher feedback loops\n",
              "*   *Contingency:* Implement human review step, create template-based fallbacks\n",
              "\n",
              "Risk 3: System Performance Too Slow\n",
              "\n",
              "*   *Impact:* User frustration, abandonment of system\n",
              "*   *Probability:* Low (15%)\n",
              "*   *Mitigation:* Regular performance testing, cloud auto-scaling\n",
              "*   *Contingency:* Optimize algorithms, upgrade infrastructure, implement queuing\n",
              "\n",
              "Medium-Risk Items:\n",
              "Risk 4: Teacher Adoption Resistance\n",
              "\n",
              "*   *Impact:* Limited user base, project success reduced\n",
              "*   *Probability:* Medium (40%)\n",
              "*   *Mitigation:* Involve teachers in design, provide extensive training\n",
              "*   *Contingency:* Adjust interface based on feedback, offer incentives\n",
              "\n",
              "# Risk 5: Data Privacy Compliance Issues \n",
              "\n",
              "Impact: Legal problems, inability to deploy in schools\n",
              "Probability: Low (20\\%)\n",
              "Mitigation: Early legal consultation, privacy-by-design approach\n",
              "Contingency: Implement additional security measures, on-premises options\n",
              "Risk 6: Technology Changes (API Changes)\n",
              "Impact: System components break, require rebuilding\n",
              "Probability: Medium (30\\%)\n",
              "Mitigation: Use stable APIs, maintain multiple provider options\n",
              "Contingency: Quick migration to alternative services, maintain local backups\n",
              "\n",
              "### 6.8 Contingency Timeline\n",
              "\n",
              "## If Major Delays Occur:3-Month Extension Plan:\n",
              "\n",
              "Focus on core content generation first (months 13-15)\n",
              "Simplify grading system to basic functionality\n",
              "Deploy minimal viable product for pilot testing\n",
              "\n",
              "### 6.9 Quality Assurance Strategy\n",
              "\n",
              "## Continuous Testing:\n",
              "\n",
              "Weekly automated testing of all system components\n",
              "Monthly user testing sessions with pilot teachers\n",
              "Quarterly security and performance audits\n",
              "\n",
              "## Success Metrics:\n",
              "\n",
              "Content generation: $90 \\%$ teacher satisfaction with quality\n",
              "Grading accuracy: $85 \\%$ correlation with human graders\n",
              "System performance: $<5$ minute processing for typical tasks\n",
              "User adoption: $80 \\%$ of pilot teachers actively using system\n",
              "\n",
              "## Feedback Integration:\n",
              "\n",
              "Bi-weekly teacher feedback sessions during beta testing\n",
              "Monthly student surveys on report quality\n",
              "Continuous monitoring of system usage analytics\n",
              "This work plan provides a realistic path to building a functional, user-friendly system while maintaining flexibility to address challenges and incorporate stakeholder feedback throughout the development process.\n",
              "\n",
              "# 7. Ethics, Legal, Data Protection and Safety Aspects \n",
              "\n",
              "### 7.1 Ethical Considerations\n",
              "\n",
              "## Fairness in AI Grading\n",
              "\n",
              "The grading system must give fair treatment to all students while grading. Teachers will provide answers or answers will be generated from the LLM with the questions and every student's answer will be matched with the stored answer. Through this we will achieve no bias. Teachers will have the final authority over the grades as the system will serve as an assistant not the replacement.\n",
              "\n",
              "## Academic Integrity\n",
              "\n",
              "The teacher will have the authority over content and quizzes to ensure academic integrity. The teacher can also generate multiple quizzes to reduce cheating.\n",
              "\n",
              "## Transparency\n",
              "\n",
              "The students and teachers will be provided a clear explanation the grading criteria and how the grades of students are calculated. A clear feedback will be provided to both teacher and students\n",
              "\n",
              "### 7.2 Legal Compliance\n",
              "\n",
              "## Educational Privacy Laws\n",
              "\n",
              "The system complies with UK GDPR and the Data Protection Act 2018, using strict access controls, audit logs, and encrypted data handling. Only authorized staff (e.g., lecturers, TAs) can access student data. Parental access is provided where legally appropriate.\n",
              "\n",
              "## International Compliance\n",
              "\n",
              "The platform supports UK and EU GDPR, ensuring rights to access, deletion, and data portability. It follows the ICO Children's Code, requiring parental consent for users under 13 and defaulting to high privacy settings for minors.\n",
              "\n",
              "### 7.3 Data Protection and Security\n",
              "\n",
              "## Data Encryption\n",
              "\n",
              "All data is encrypted in transit (HTTPS) and at rest (AES-256). Student personally identifiable information receives the highest protection level with field-level encryption in databases.\n",
              "\n",
              "## Access Controls\n",
              "\n",
              "Role-based access ensures students only see their own data, teachers access their class information, and administrators have limited system-wide access. Multi-factor authentication is required for all accounts.\n",
              "\n",
              "## Data Retention\n",
              "\n",
              "Student data is retained only as long as educationally necessary, with automatic deletion after course completion unless explicitly retained by the institution. Clear data retention policies align with institutional requirements.\n",
              "\n",
              "### 7.4 Safety and Reliability\n",
              "\n",
              "# System Reliability \n",
              "\n",
              "The system maintains $99.5 \\%$ uptime with automated backups and disaster recovery procedures. Multiple server regions ensure continued operation during outages.\n",
              "\n",
              "## Content Safety\n",
              "\n",
              "AI-generated content is filtered for appropriateness, with human review mechanisms for questionable material. The system includes reporting tools for inappropriate content.\n",
              "\n",
              "## Error Handling\n",
              "\n",
              "Clear error messages help users understand system limitations. Low-confidence OCR results are flagged for human review, and technical failures trigger automatic notifications to administrators.\n",
              "\n",
              "### 7.5 User Rights and Protections\n",
              "\n",
              "## Student Rights\n",
              "\n",
              "Students can request to see their data, correct errors, and opt out of AI grading in favor of human-only assessment. Parents can access their children's educational records and system usage.\n",
              "\n",
              "## Teacher Autonomy\n",
              "\n",
              "Teachers retain complete control over their educational content and grading decisions. The system cannot override teacher judgment or automatically assign final grades without approval.\n",
              "\n",
              "## Institutional Control\n",
              "\n",
              "Educational institutions maintain ownership of their data and can export or delete all information at any time. Local deployment options are available for sensitive environments.\n",
              "\n",
              "This framework ensures the system enhances education while protecting all stakeholders' rights, privacy, and academic integrity.\n",
              "\n",
              "## References\n",
              "\n",
              "Alheraki, F., Alnuaim, A., \\& Almudaires, N. (2023). Handwritten Arabic character recognition for children writing using convolutional neural network and stroke identification. Human-Centric Intelligent Systems. Handwritten Arabic Character Recognition for Children Writing Using Convolutional Neural Network and Stroke Identification | Human-Centric Intelligent Systems.\n",
              "\n",
              "Doughty, C., Hussein, M., \\& Li, S. (2023). A comparative study of AI-generated (GPT-4) and human-crafted MCQs in programming education. arXiv preprint [arXiv:2312.03173].\n",
              "https://arxiv.org/abs/2312.03173\n",
              "Elkins, D., Rivera, L., \\& Martin, K. (2024). How teachers can use large language models and Bloom's taxonomy to create educational quizzes. arXiv preprint [arXiv:2401.05914].\n",
              "https://arxiv.org/abs/2401.05914\n",
              "Education Endowment Foundation (EEF). (2021). Teacher feedback to improve pupil learning: Guidance report. Teacher Feedback to Improve Pupil Learning | EEF\n",
              "\n",
              "HESA (2023). Higher Education Staff Statistics: UK, 2021/22. Higher Education Statistics Agency. https://www.hesa.ac.uk/news/23-02-2023/staff-statistics\n",
              "\n",
              "Mohan, M., Kamath, S., \\& Chakrabarti, S. (2021). D2S: Document-to-slide generation via query-based text summarization. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP). ACL Anthology. Improving Simultaneous Translation by Incorporating Pseudo-References with Fewer Reorderings - ACL Anthology\n",
              "\n",
              "Safilian, S., Baines, M., \\& Anvari, A. (2025). RATAS framework: A comprehensive genAI-based approach to rubric-based marking of real-world textual exams. arXiv preprint [arXiv:2505.23818]. https://arxiv.org/abs/2505.23818\n",
              "\n",
              "Shreewastav, A., Joshi, A., \\& Das, P. (2024). Presently: Automated presentation slide generation from research papers using NLP and deep learning. Preprint. (PDF) Presently: Automated Presentation Slide Generation from Research Papers using NLP and Deep Learning (May 2024)\n",
              "\n",
              "Gao, Y., Chang, L., \\& Uddin, M. (2023). Automatic assessment of text-based responses in post-secondary education: A systematic review. arXiv preprint [arXiv:2308.16151].\n",
              "https://arxiv.org/abs/2308.16151\n",
              "Ozan, B., Filiz, O., \\& Kaya, R. (2025). Teachers and AI: Understanding the factors influencing AI integration in K-12 education. Education and Information Technologies. Teachers and AI: Understanding the factors influencing AI integration in K-12 education I Education and Information Technologies\n",
              "\n",
              "Simon Fraser University. (n.d.). Strategies to reduce lecture preparation time. Institute for the Study of Teaching and Learning in the Disciplines (ISTLD), SFU. Retrieved July 4, 2025, from Ideas to Reduce Non-Classroom Prep Time Without Reducing Teaching Quality - ISTLD - Simon Fraser University\n",
              "\n",
              "Yale Graduate Teaching Center. (2013). The diminishing returns of lecture preparation. Yale University. Retrieved July 4, 2025, from How to spend less time preparing better lectures"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from mistralai.models import OCRResponse\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "\n",
        "def replace_images_in_markdown(markdown_str: str, images_dict: dict) -> str:\n",
        "    \"\"\"\n",
        "    Replace image placeholders in markdown with base64-encoded images.\n",
        "\n",
        "    Args:\n",
        "        markdown_str: Markdown text containing image placeholders\n",
        "        images_dict: Dictionary mapping image IDs to base64 strings\n",
        "\n",
        "    Returns:\n",
        "        Markdown text with images replaced by base64 data\n",
        "    \"\"\"\n",
        "    for img_name, base64_str in images_dict.items():\n",
        "        markdown_str = markdown_str.replace(\n",
        "            f\"![{img_name}]({img_name})\", f\"![{img_name}]({base64_str})\"\n",
        "        )\n",
        "    return markdown_str\n",
        "\n",
        "\n",
        "def get_combined_markdown(ocr_response: OCRResponse) -> str:\n",
        "    \"\"\"\n",
        "    Combine OCR text and images into a single markdown document.\n",
        "\n",
        "    Args:\n",
        "        ocr_response: Response from OCR processing containing text and images\n",
        "\n",
        "    Returns:\n",
        "        Combined markdown string with embedded images\n",
        "    \"\"\"\n",
        "    markdowns: list[str] = []\n",
        "    # Extract images from page\n",
        "    for page in ocr_response.pages:\n",
        "        image_data = {}\n",
        "        for img in page.images:\n",
        "            image_data[img.id] = img.image_base64\n",
        "        # Replace image placeholders with actual images\n",
        "        markdowns.append(replace_images_in_markdown(page.markdown, image_data))\n",
        "\n",
        "    return \"\\n\\n\".join(markdowns)\n",
        "\n",
        "\n",
        "# Display combined markdowns and images\n",
        "display(Markdown(get_combined_markdown(pdf_response)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_v1 = \"\"\"\n",
        "<instructions>\n",
        "You are a \"Quiz Content Normalizer\" agent. Your primary task is to process Markdown-formatted quiz content, extract the questions and student answers, and convert this information into a structured JSON object. A critical secondary task is to identify and remove \"teacher remarks\" from the content. These remarks are feedback or corrections provided by an instructor and should NOT be included in the final JSON output.\n",
        "\n",
        "You must be highly intelligent and discerning in identifying teacher remarks. They are distinct from the student's actual answers and the original question statements. Your goal is to preserve the complete and accurate student responses while eliminating only the teacher's annotations.\n",
        "</instructions>\n",
        "\n",
        "<input_format>\n",
        "The input will be raw Markdown text representing a quiz. It will contain questions (e.g., \"a. Is the relation in 2NF?...\") and the student's written answers. The original source had teacher remarks visually marked in red, but this visual cue is LOST in the Markdown conversion. You must identify them based on textual patterns and context.\n",
        "</input_format>\n",
        "\n",
        "<output_format>\n",
        "Produce a JSON array of objects. Each object in the array should represent a single question-answer pair from the quiz.\n",
        "Each object must have the following structure:\n",
        "```json\n",
        "{\n",
        "  \"question_number\": \"string\", // e.g., \"a\", \"b\", \"c\"\n",
        "  \"question_text\": \"string\",   // The full text of the question\n",
        "  \"student_answer\": \"string\"   // The student's complete answer, with ALL teacher remarks removed.\n",
        "}\n",
        "```\n",
        "</output_format>\n",
        "\n",
        "<guidelines_for_identifying_teacher_remarks>\n",
        "Teacher remarks are typically short, evaluative, corrective, or annotative comments. They are NOT part of the student's intended answer or the original question. Apply the following heuristics carefully:\n",
        "\n",
        "1.  **Short, Non-Syntactic Lines/Phrases:** Look for very short lines or phrases (e.g., under 20-30 characters) that appear as interjections, marginalia, or brief feedback, and do not form a grammatically coherent part of the student's main explanation.\n",
        "2.  **Evaluative/Corrective Language:** Identify phrases containing direct feedback, judgment, or correction. Common indicators include:\n",
        "    *   \"not valid\", \"not in XNF\" (when used as a standalone correction, not part of student's argument), \"sorry :|\", \"do not\", \"wrong\", \"correct\", \"no\", \"yes\", \"partial dependency exists\" (when used as a direct correction to a student's incorrect statement).\n",
        "    *   Phrases like \"How can it be 3NF if it is not even 2NF?\"\n",
        "    *   Phrases like \"no decomposition.\", \"no yes/no\", \"functional dependency this wouldn't be a relation if there was not any functional dependencies.\"\n",
        "    *   Phrases like \"These new table will be created\" (as an instruction/correction).\n",
        "3.  **Numerical/Symbolic Annotations:** Lines consisting solely of a single digit, a circled digit (e.g., 'â‘ ', 'â‘¡', 'â“ª'), or simple symbols like 'X' or 'âœ“' that indicate a mark or score.\n",
        "4.  **Syntactic Disruption:** Remarks often appear to break the flow of the student's answer, either on a new line or as an inserted comment that doesn't logically connect to the surrounding student text.\n",
        "\n",
        "**CRITICAL EXCEPTION:** Be extremely careful not to remove legitimate parts of the student's answer that might *contain* words like \"no\" or \"not\" if they are part of their technical explanation (e.g., \"The relation is **not** in 2NF because **no** partial dependency exists...\"). Distinguish between student's reasoning and teacher's direct feedback. If a phrase like \"no partial dependency exists\" is presented as a student's justification, it should be kept. If it's a short, isolated correction from the teacher, it should be removed. Context is key.\n",
        "</guidelines_for_identifying_teacher_remarks>\n",
        "\n",
        "<chain_of_thought>\n",
        "Before generating the JSON, perform the following steps mentally or explicitly:\n",
        "1.  **Parse Quiz Structure:** Identify the distinct sections for each question (a, b, c, d...).\n",
        "2.  **Extract Question Text:** For each section, isolate the exact question statement.\n",
        "3.  **Extract Raw Student Answer:** Collect all text associated with the student's response for that question.\n",
        "4.  **Filter Teacher Remarks:** Carefully review the raw student answer text. Apply the `guidelines_for_identifying_teacher_remarks` to remove all identified remarks. This step requires nuanced understanding to differentiate between student content and teacher annotations.\n",
        "5.  **Construct JSON:** Assemble the extracted question number, question text, and the cleaned student answer into the specified JSON format.\n",
        "</chain_of_thought>\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yk5tBpPuKal"
      },
      "source": [
        "Cleaning up\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aZOQs38r0GO",
        "outputId": "3d406569-57bc-4efb-cc92-e9a670f97ed7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"id\": \"3e61ea3d72274a5da15f526cf4838b25\",\n",
            "    \"object\": \"chat.completion\",\n",
            "    \"model\": \"ministral-8b-latest\",\n",
            "    \"usage\": {\n",
            "        \"prompt_tokens\": 666,\n",
            "        \"completion_tokens\": 448,\n",
            "        \"total_tokens\": 1114\n",
            "    },\n",
            "    \"created\": 1753165301,\n",
            "    \"choices\": [\n",
            "        {\n",
            "            \"index\": 0,\n",
            "            \"message\": {\n",
            "                \"content\": \"{\\n  \\\"quiz_number\\\": 6,\\n  \\\"name\\\": \\\"M. Ahmed Riaz\\\",\\n  \\\"roll_number\\\": \\\"23I-2005\\\",\\n  \\\"questions\\\": [\\n    {\\n      \\\"id\\\": \\\"q1\\\",\\n      \\\"text\\\": \\\"Is the relation in 2NF? Provide the reason if it is not in 2NF. [2 marks] The relation is not in 2NF, because in there is partial dependency exists in Functional Dependency. FD1. We have to create those table to be in 2NF.\\\",\\n      \\\"type\\\": \\\"open_ended\\\",\\n      \\\"options\\\": [],\\n      \\\"parts\\\": []\\n    },\\n    {\\n      \\\"id\\\": \\\"q2\\\",\\n      \\\"text\\\": \\\"Is the relation in 3NF? Provide the reason if it is not in 3NF. [2 marks] The relation is not in 3NF form because there is a transitive relation in FD1 to FD4. Member ID \\u2192 Loan_Date, Loan_Date \\u2192 X (Not Valid) not in 3NF How can it be 3NF if it is not even 2NF?\\\",\\n      \\\"type\\\": \\\"open_ended\\\",\\n      \\\"options\\\": [],\\n      \\\"parts\\\": []\\n    },\\n    {\\n      \\\"id\\\": \\\"q3\\\",\\n      \\\"text\\\": \\\"Convert the relation to 2NF if it is not in 2NF. If it is, state that it is in 2NF. [3 marks] Member ID \\u2192 Loan_Date ? These new table will be created Book_ISBN \\u2192 Return_Date ? No decompilation\\\",\\n      \\\"type\\\": \\\"open_ended\\\",\\n      \\\"options\\\": [],\\n      \\\"parts\\\": []\\n    },\\n    {\\n      \\\"id\\\": \\\"q4\\\",\\n      \\\"text\\\": \\\"Convert the relation in 3NF if it is not in 3NF. If it is, state it is in 3NF [3 marks] After converting it into 3NF, it will be in 3NF form as there will be no transitive dependency. Only Full and Partial will exist.\\\",\\n      \\\"type\\\": \\\"open_ended\\\",\\n      \\\"options\\\": [],\\n      \\\"parts\\\": []\\n    }\\n  ]\\n}\",\n",
            "                \"tool_calls\": null,\n",
            "                \"prefix\": false,\n",
            "                \"role\": \"assistant\",\n",
            "                \"parsed\": {\n",
            "                    \"quiz_number\": 6,\n",
            "                    \"name\": \"M. Ahmed Riaz\",\n",
            "                    \"roll_number\": \"23I-2005\",\n",
            "                    \"questions\": [\n",
            "                        {\n",
            "                            \"id\": \"q1\",\n",
            "                            \"text\": \"Is the relation in 2NF? Provide the reason if it is not in 2NF. [2 marks] The relation is not in 2NF, because in there is partial dependency exists in Functional Dependency. FD1. We have to create those table to be in 2NF.\",\n",
            "                            \"type\": \"open_ended\",\n",
            "                            \"options\": [],\n",
            "                            \"parts\": []\n",
            "                        },\n",
            "                        {\n",
            "                            \"id\": \"q2\",\n",
            "                            \"text\": \"Is the relation in 3NF? Provide the reason if it is not in 3NF. [2 marks] The relation is not in 3NF form because there is a transitive relation in FD1 to FD4. Member ID \\u2192 Loan_Date, Loan_Date \\u2192 X (Not Valid) not in 3NF How can it be 3NF if it is not even 2NF?\",\n",
            "                            \"type\": \"open_ended\",\n",
            "                            \"options\": [],\n",
            "                            \"parts\": []\n",
            "                        },\n",
            "                        {\n",
            "                            \"id\": \"q3\",\n",
            "                            \"text\": \"Convert the relation to 2NF if it is not in 2NF. If it is, state that it is in 2NF. [3 marks] Member ID \\u2192 Loan_Date ? These new table will be created Book_ISBN \\u2192 Return_Date ? No decompilation\",\n",
            "                            \"type\": \"open_ended\",\n",
            "                            \"options\": [],\n",
            "                            \"parts\": []\n",
            "                        },\n",
            "                        {\n",
            "                            \"id\": \"q4\",\n",
            "                            \"text\": \"Convert the relation in 3NF if it is not in 3NF. If it is, state it is in 3NF [3 marks] After converting it into 3NF, it will be in 3NF form as there will be no transitive dependency. Only Full and Partial will exist.\",\n",
            "                            \"type\": \"open_ended\",\n",
            "                            \"options\": [],\n",
            "                            \"parts\": []\n",
            "                        }\n",
            "                    ]\n",
            "                }\n",
            "            },\n",
            "            \"finish_reason\": \"stop\"\n",
            "        }\n",
            "    ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "from pydantic import BaseModel\n",
        "\n",
        "\n",
        "class QuizQuestion(BaseModel):\n",
        "    id: str\n",
        "    text: str\n",
        "    type: str\n",
        "    options: list[dict] = []\n",
        "    parts: list[dict] = []\n",
        "\n",
        "class StructuredQuiz(BaseModel):\n",
        "    quiz_number: int\n",
        "    name: str\n",
        "    roll_number: str\n",
        "    questions: list[QuizQuestion]\n",
        "\n",
        "prompt = (\n",
        "    \"You are an expert JSON conversion agent. \"\n",
        "    \"Convert the markdown quiz provided in `<markdown_quiz>` tags into a JSON object. \"\n",
        "    \"Adhere strictly to the specified schema, ensuring consistency while accommodating variations in question counts, sub-points, and multiple-choice formats. \"\n",
        "    \"Infer question `type` (e.g., `multiple_choice`, `open_ended`) based on content.\\n\\n\"\n",
        "    \"Output must conform to this JSON Schema:\\n\"\n",
        "    \"{\\n\"\n",
        "    \"  \\\"quiz_number\\\": \\\"integer\\\",\\n\"\n",
        "    \"  \\\"name\\\": \\\"string\\\",\\n\"\n",
        "    \"  \\\"roll_number\\\": \\\"string\\\",\\n\"\n",
        "    \"  \\\"questions\\\": [\\n\"\n",
        "    \"    {\\n\"\n",
        "    \"      \\\"id\\\": \\\"string\\\",\\n\"\n",
        "    \"      \\\"text\\\": \\\"string\\\",\\n\"\n",
        "    \"      \\\"type\\\": \\\"string\\\",\\n\"\n",
        "    \"      \\\"options\\\": [ {\\\"label\\\": \\\"string\\\", \\\"text\\\": \\\"string\\\"} ],\\n\"\n",
        "    \"      \\\"parts\\\": [ {\\\"label\\\": \\\"string\\\", \\\"text\\\": \\\"string\\\"} ]\\n\"\n",
        "    \"    }\\n\"\n",
        "    \"  ]\\n\"\n",
        "    \"}\\n\"\n",
        ")\n",
        "\n",
        "quiz_markdown = pdf_response.pages[0].markdown\n",
        "\n",
        "# Compose the chat request\n",
        "chat_response = client.chat.parse(\n",
        "    model=\"ministral-8b-latest\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                TextChunk(\n",
        "                    text=(\n",
        "                        f\"{prompt}\\n\"\n",
        "                        \"<markdown_quiz>\\n\"\n",
        "                        f\"{quiz_markdown}\\n\"\n",
        "                        \"</markdown_quiz>\"\n",
        "                    )\n",
        "                ),\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        "    response_format=StructuredQuiz,\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "\n",
        "def extract_quiz(response):\n",
        "    # If response is a dict and has OpenAI/Mistral completion keys:\n",
        "    if isinstance(response, dict):\n",
        "        content = response.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", None)\n",
        "        if content:\n",
        "            try:\n",
        "                return json.loads(content)\n",
        "            except Exception:\n",
        "                return content  # It may already be a dict\n",
        "    # If response is a Pydantic model:\n",
        "    if hasattr(response, 'model_dump'):\n",
        "        return response.model_dump()\n",
        "    # If already dict:\n",
        "    if isinstance(response, dict):\n",
        "        return response\n",
        "    # Fallback: just return as-is\n",
        "    return response\n",
        "\n",
        "quiz_json = extract_quiz(chat_response)\n",
        "print(json.dumps(quiz_json, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def is_valid_quiz_format(response_dict):\n",
        "    # Top-level keys and types\n",
        "    if not isinstance(response_dict, dict):\n",
        "        return False\n",
        "    for key in [\"quiz_number\", \"name\", \"roll_number\", \"questions\"]:\n",
        "        if key not in response_dict:\n",
        "            return False\n",
        "    if not isinstance(response_dict[\"questions\"], list):\n",
        "        return False\n",
        "    # Each question object\n",
        "    for q in response_dict[\"questions\"]:\n",
        "        if not isinstance(q, dict):\n",
        "            return False\n",
        "        for key in [\"id\", \"text\", \"type\", \"parts\"]:\n",
        "            if key not in q:\n",
        "                return False\n",
        "        if \"options\" in q and not isinstance(q[\"options\"], list):\n",
        "            return False\n",
        "        if not isinstance(q[\"parts\"], list):\n",
        "            return False\n",
        "        # Option objects (if present)\n",
        "        if \"options\" in q:\n",
        "            for opt in q[\"options\"]:\n",
        "                if not isinstance(opt, dict):\n",
        "                    return False\n",
        "                if \"label\" not in opt or \"text\" not in opt:\n",
        "                    return False\n",
        "        # Part objects (if present)\n",
        "        for part in q[\"parts\"]:\n",
        "            if not isinstance(part, dict):\n",
        "                return False\n",
        "            if \"label\" not in part or \"text\" not in part:\n",
        "                return False\n",
        "    return True\n",
        "\n",
        "max_attempts = 5\n",
        "attempt = 0\n",
        "while attempt < max_attempts:\n",
        "    attempt += 1\n",
        "    pdf_markdown = pdf_response.pages[0].markdown\n",
        "\n",
        "    chat_response = client.chat.complete(\n",
        "        model=\"ministral-8b-latest\",\n",
        "        messages=[\n",
        "            {   # type: ignore\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    TextChunk(\n",
        "                        text=f\"{prompt}\\n<markdown_quiz>\\n{pdf_markdown}\\n</markdown_quiz>\"\n",
        "                    ),\n",
        "                ],\n",
        "            }\n",
        "        ],\n",
        "        response_format={\"type\": \"json_object\"},\n",
        "        temperature=0,\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response_dict = json.loads(chat_response.choices[0].message.content)\n",
        "        if is_valid_quiz_format(response_dict):\n",
        "            print(f\"VALID: {json.dumps(response_dict, indent=4)}\")\n",
        "        else:\n",
        "            print(f\"Attempt {attempt}: Output format invalid, retrying...\")\n",
        "            print(json.dumps(response_dict, indent=4))\n",
        "    except Exception as e:\n",
        "        print(f\"Attempt {attempt}: JSON parsing failed, retrying... {e}\")\n",
        "    time.sleep(1)\n",
        "else:\n",
        "    print(\"Failed to get valid output after several attempts.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "mistral",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
